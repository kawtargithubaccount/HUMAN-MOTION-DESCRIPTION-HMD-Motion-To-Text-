{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":92710,"databundleVersionId":11054894,"sourceType":"competition"}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n## **<p style=\"text-align: center; text-decoration: underline;\">DATA CHALLENGE</p>**\n# **<p style=\"text-align: center;\">HUMAN MOTION DESCRIPTION (HMD): Motion-To-Text</p>**\n---\n\n> *2025*.\n\n---\n\n![examples](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimg.clipart-library.com%2F2%2Fclip-motions%2Fclip-motions-6.png&f=1&nofb=1&ipt=0747ffa645bb5f7798e8a2d44499b28f1156ce0e83b1b300fabfed4c6ab1fdf2&ipo=images)\n\n### ■ **Overview**\nIn this data challenge, you will explore the intersection of natural language processing (NLP) and human motion synthesis by working on text-to-motion and motion-to-text tasks using the HumanML3D dataset. This dataset contains 3D human motion sequences paired with rich textual descriptions, enabling models to learn bidirectional mappings between language and motion.\n\n#### **I. Main Task: Motion-To-Text & Text-to-Motion Generation**\n- **Motion-to-Text:** Develop a model to describe human motions in natural language given a sequence of 3D poses.\n\n#### **II. Dataset Overview:**\n- HumanML3D includes 14,616 motion samples across diverse actions (walking, dancing, sports) and 44,970 text annotations.\n- Data includes skeletal joint positions, rotations, and fine-grained textual descriptions.\n\n<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fproduction-media.paperswithcode.com%2Fdatasets%2F446194c5-ce59-43eb-b4cb-570a7a4d0cd9.png&f=1&nofb=1&ipt=b2edbe3251cab88e26a7f9d4e765c811b2cc890dc2ace7f7456baeca076b115b&ipo=images\" alt=\"description\" style=\"width:800px; height:600px;\" />\n\nThe provided dataset contains the following components:\n\n- 1. `motions` Folder: Contains `.npy` files, each representing a sequence of body poses. Each file has a shape of `(T, N, d)`, where:\n  - `T`: Number of frames in the sequence (varies across sequences).\n  - `N`: Number of joints in the body (22 in this case).\n  - `d`: Dimension of each joint (3D coordinates: `x`, `y`, `z`).\n\n- 2. `texts` Folder: Contains `.npy` files, each providing **3/4 textual descriptions** of the corresponding motion sequence. Each description is accompanied by part-of-speech (POS) tags for every word in the description. Example: \"a person jump hop to the right#a/DET person/NOUN jump/NOUN hop/NOUN to/ADP the/DET right/NOUN#\"\n\n- 3. File Lists\n    - **`train.txt`**: List of motion files for training.\n    - **`val.txt`**: List of motion files for validation.\n    - **`test.txt`**: List of motion files for testing.\n\n#### **III. Evaluation Metrics**\n\n**Similarity Score:** computes the similarity score between the predicted text and ground truth texts.\n> Note: Higher similarity (closer to 1 or 100\\%) indicate better text-motion alignment.\n\nSolutions should be submitted in the following format (in a csv file):\n\nFor each ID in the motion test set (`test.txt`), you must predict the corresponding description. The file should contain a header and have the following format:\n\n| id      | text                                                                 |\n|---------|---------------------------------------------------------------------|\n| 004822  | A person walks slowly forward, swinging their arms naturally        |\n| 014457  | Someone performs a golf swing with proper form                      |\n| 009613  | An individual jogs backwards diagonally across the room             |\n| 008463  | A man bends down to pick up an object while walking                 |\n| 012365  | A dancer spins clockwise while raising both arms                    |\n| 007933  | Two people engage in a slow-motion martial arts demonstration       |\n| 003430  | A child skips happily across a playground                           |\n| 014522  | An athlete performs a perfect cartwheel sequence                    |\n| 005698  | A woman gracefully practices yoga sun salutations                   |\n| 001664  | A parkour expert vaults over a low wall                             |\n\nYou can generate your submission files using pandas as follows:\n\n    >>> submission = pd.DataFrame({\n    ...     'id': ['004822', '014457', ...],\n    ...     'text': [\n    ...         \"a person walking slowly\",\n    ...         \"someone swinging a golf club\",\n    ...         ...\n    ...     ]\n    ... })\n    ... submission.to_csv('./submission.csv', index=False)\n    \n#### **References**\n\n- Jiang, B., Chen, X., Liu, W., Yu, J., Yu, G., & Chen, T. (2023). Motiongpt: Human motion as a foreign language. Advances in Neural Information Processing Systems, 36, 20067-20079.\n- Zhu, W., Ma, X., Ro, D., Ci, H., Zhang, J., Shi, J., ... & Wang, Y. (2023). Human motion generation: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n- Xu, L., Song, Z., Wang, D., Su, J., Fang, Z., Ding, C., ... & Wu, W. (2023). Actformer: A gan-based transformer towards general action-conditioned 3d human motion generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 2228-2238).","metadata":{"tags":[]}},{"cell_type":"markdown","source":"### **Animation Demo**","metadata":{}},{"cell_type":"code","source":"import os\nfrom os.path import join as pjoin\nfrom tqdm import tqdm\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nimport mpl_toolkits.mplot3d.axes3d as p3\n\n# Define the kinematic tree for connecting joints\nkinematic_tree = [\n    [0, 2, 5, 8, 11], \n    [0, 1, 4, 7, 10], \n    [0, 3, 6, 9, 12, 15], \n    [9, 14, 17, 19, 21], \n    [9, 13, 16, 18, 20]\n]\n\ndef plot_3d_motion(save_path, joints, title, figsize=(10, 10), fps=120, radius=4):\n    # Split the title if it's too long\n    title_sp = title.split(' ')\n    if len(title_sp) > 10:\n        title = '\\n'.join([' '.join(title_sp[:10]), ' '.join(title_sp[10:])])\n\n    def init():\n        ax.set_xlim3d([-radius / 2, radius / 2])\n        ax.set_ylim3d([0, radius])\n        ax.set_zlim3d([0, radius])\n        fig.suptitle(title, fontsize=20)\n        ax.grid(b=False)\n\n    def plot_xzPlane(minx, maxx, miny, minz, maxz):\n        # Plot a plane XZ\n        verts = [\n            [minx, miny, minz],\n            [minx, miny, maxz],\n            [maxx, miny, maxz],\n            [maxx, miny, minz]\n        ]\n        xz_plane = Poly3DCollection([verts])\n        xz_plane.set_facecolor((0.5, 0.5, 0.5, 0.5))\n        ax.add_collection3d(xz_plane)\n\n    # Reshape the joints data\n    data = joints.copy().reshape(len(joints), -1, 3)\n    # fig = plt.figure(figsize=figsize)\n    # ax = p3.Axes3D(fig)\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(111, projection='3d')\n    init()\n\n    # Compute min and max values for the data\n    MINS = data.min(axis=0).min(axis=0)\n    MAXS = data.max(axis=0).max(axis=0)\n\n    # Define colors for the kinematic tree\n    colors = ['red', 'blue', 'black', 'red', 'blue',  \n              'darkblue', 'darkblue', 'darkblue', 'darkblue', 'darkblue',\n              'darkred', 'darkred', 'darkred', 'darkred', 'darkred']\n\n    frame_number = data.shape[0]\n\n    # Adjust the height offset\n    height_offset = MINS[1]\n    data[:, :, 1] -= height_offset\n    trajec = data[:, 0, [0, 2]]\n\n    # Center the data\n    data[..., 0] -= data[:, 0:1, 0]\n    data[..., 2] -= data[:, 0:1, 2]\n\n    def update(index):\n        # Clear existing lines and collections\n        for line in ax.lines:\n            line.remove()\n        for collection in ax.collections:\n            collection.remove()\n\n        # Update the view\n        ax.view_init(elev=120, azim=-90)\n        ax.dist = 7.5\n\n        # Plot the XZ plane\n        plot_xzPlane(MINS[0] - trajec[index, 0], MAXS[0] - trajec[index, 0], 0, MINS[2] - trajec[index, 1], MAXS[2] - trajec[index, 1])\n\n        # Plot the trajectory\n        if index > 1:\n            ax.plot3D(trajec[:index, 0] - trajec[index, 0], np.zeros_like(trajec[:index, 0]), trajec[:index, 1] - trajec[index, 1], linewidth=1.0, color='blue')\n\n        # Plot the kinematic tree\n        for i, (chain, color) in enumerate(zip(kinematic_tree, colors)):\n            linewidth = 4.0 if i < 5 else 2.0\n            ax.plot3D(data[index, chain, 0], data[index, chain, 1], data[index, chain, 2], linewidth=linewidth, color=color)\n        # Hide axis labels\n        plt.axis('off')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_zticklabels([])\n\n    # Create the animation\n    ani = FuncAnimation(fig, update, frames=frame_number, interval=1000 / fps, repeat=False)\n\n    # Save the animation\n    ani.save(save_path, fps=fps)\n    plt.close()\n\n    print(f'Animation saved to {save_path}!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T15:54:41.293305Z","iopub.execute_input":"2025-02-13T15:54:41.293656Z","iopub.status.idle":"2025-02-13T15:54:41.308565Z","shell.execute_reply.started":"2025-02-13T15:54:41.293607Z","shell.execute_reply":"2025-02-13T15:54:41.307409Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"## /!\\ attention ! travaux: path to data -> replace this with your own paths\nmotion_data_dir = '/kaggle/input/human-motion-description-hmd-motion-to-text/motions/'\ntext_data_dir = '/kaggle/input/human-motion-description-hmd-motion-to-text/texts/' \n\n## list all files in the folder\nnpy_files = sorted(os.listdir(motion_data_dir))\n\n## pick a random motion file\nnpy_file = np.random.choice(npy_files)\n\n## read npy motion file\nmotion_data = np.load(os.path.join(motion_data_dir, npy_file))\nprint('shape', motion_data.shape)\n\n## get the corresponding titles for the given motion\ntitles = []\nwith open('{}{}.txt'.format(text_data_dir, npy_file.split('.')[0])) as f:\n    descriptions = f.readlines()\n    for desc in descriptions:\n        titles.append(desc.split('#')[0].capitalize())\n\nprint('Descriptions:')\nprint('- '+'\\n- '.join(titles))\n\n## pick a random title\ntitle = np.random.choice(titles)\n\n## create & save animation\nsave_path = './animation.gif'\nplot_3d_motion(save_path, motion_data, title=title, figsize=(10, 6), fps=30, radius=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T15:54:43.416672Z","iopub.execute_input":"2025-02-13T15:54:43.416963Z","iopub.status.idle":"2025-02-13T15:54:48.674254Z","shell.execute_reply.started":"2025-02-13T15:54:43.416940Z","shell.execute_reply":"2025-02-13T15:54:48.673457Z"}},"outputs":[{"name":"stdout","text":"shape (100, 22, 3)\nDescriptions:\n- A man walks up steps with his left hand on the railing.\n- A person slowly walked upstairs\n- While holding on to a rail with his left hand a person climbs up stairs.\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-c50be989e98c>:84: MatplotlibDeprecationWarning: The dist attribute was deprecated in Matplotlib 3.6 and will be removed two minor releases later.\n  ax.dist = 7.5\n","output_type":"stream"},{"name":"stdout","text":"Animation saved to ./animation.gif!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\n\ndef extract_and_stack_frames(gif_path):\n    \"\"\"\n    Extract frames from a GIF, preprocess them, and return a stacked tensor.\n    \n    Args:\n    - gif_path (str): Path to the input GIF file.\n    \n    Returns:\n    - frames_tensor (Tensor): Tensor of shape (T, 3, 224, 224), where:\n        - T = number of frames\n        - 3 = RGB channels\n        - 224x224 = Resized image for ClipBERT\n    \"\"\"\n    gif = Image.open(gif_path)\n    frames = []\n\n    # Define the transformation (Resize + Normalize)\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  # Resize for ClipBERT\n        transforms.ToTensor(),  # Convert to tensor\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n    ])\n\n    while True:\n        # Convert frame to RGB and apply transformation\n        frame = gif.convert(\"RGB\")\n        frame_tensor = transform(frame)\n        frames.append(frame_tensor)\n        \n        try:\n            gif.seek(gif.tell() + 1)  # Move to the next frame\n        except EOFError:\n            break  # No more frames\n\n    # Stack frames along the batch dimension\n    frames_tensor = torch.stack(frames)  # Shape: (T, 3, 224, 224)\n\n    return frames_tensor\n    \n# Example usage\ngif_path = \"animation.gif\"\nframes  = extract_and_stack_frames(gif_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T07:39:40.91328Z","iopub.execute_input":"2025-02-13T07:39:40.913573Z","iopub.status.idle":"2025-02-13T07:39:41.754952Z","shell.execute_reply.started":"2025-02-13T07:39:40.913549Z","shell.execute_reply":"2025-02-13T07:39:41.754251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataset_dataloader import *\ndata_dir = '/kaggle/input/human-motion-description-hmd-motion-to-text/'\ntrain_set = MotionDataset(data_dir, 'train.txt', mean=None, std=None)\nvalid_set = MotionDataset(data_dir, 'val.txt', mean=None, std=None)\n\nbatch_size = 64\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size)\n\n\nfor motion, text in train_loader:\n    print('motion shape:', motion.shape)\n    print('exemple of texts:', text[0])\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T15:54:49.302825Z","iopub.execute_input":"2025-02-13T15:54:49.303126Z","iopub.status.idle":"2025-02-13T15:58:03.040822Z","shell.execute_reply.started":"2025-02-13T15:54:49.303104Z","shell.execute_reply":"2025-02-13T15:58:03.039615Z"}},"outputs":[{"name":"stderr","text":"loading data...: 100%|██████████| 13012/13012 [02:32<00:00, 85.20it/s]\nloading data...: 100%|██████████| 3254/3254 [00:37<00:00, 86.82it/s]","output_type":"stream"},{"name":"stdout","text":"motion shape: torch.Size([64, 100, 22, 3])\nexemple of texts: person scratches their face with left hand then right hand\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define the kinematic tree\nkinematic_tree = [\n    [0, 2, 5, 8, 11], \n    [0, 1, 4, 7, 10], \n    [0, 3, 6, 9, 12, 15], \n    [9, 14, 17, 19, 21], \n    [9, 13, 16, 18, 20]\n]\n\n# Number of joints\nnum_joints = 22\n\n# Build adjacency matrix\nadj_matrix = np.zeros((num_joints, num_joints), dtype=np.float32)\nfor branch in kinematic_tree:\n    for i in range(len(branch) - 1):\n        adj_matrix[branch[i], branch[i + 1]] = 1\n        adj_matrix[branch[i + 1], branch[i]] = 1  # Undirected graph\n\n# Convert to PyTorch tensor\nadj_matrix = torch.tensor(adj_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T15:58:03.055973Z","iopub.execute_input":"2025-02-13T15:58:03.056200Z","iopub.status.idle":"2025-02-13T15:58:03.069929Z","shell.execute_reply.started":"2025-02-13T15:58:03.056181Z","shell.execute_reply":"2025-02-13T15:58:03.068948Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## **GCN basic**","metadata":{}},{"cell_type":"code","source":"class GCNEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n        super(GCNEncoder, self).__init__()\n        self.adj_matrix = adj_matrix\n        self.gcn1 = nn.Linear(input_dim, hidden_dim)\n        self.gcn2 = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        # x shape: (batch_size, T, N, d)\n        batch_size, T, N, d = x.shape\n        \n        # Reshape for GCN: (batch_size * T, N, d)\n        x = x.view(-1, N, d)\n        \n        # GCN Layer 1\n        x = F.relu(torch.matmul(self.adj_matrix, self.gcn1(x)))\n        \n        # GCN Layer 2\n        x = torch.matmul(self.adj_matrix, self.gcn2(x))\n        \n        # Reshape back: (batch_size, T, N, output_dim)\n        x = x.view(batch_size, T, N, -1)\n        \n        # Aggregate over joints and time: (batch_size, output_dim)\n        x = x.mean(dim=1).mean(dim=1)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:41:35.321115Z","iopub.execute_input":"2025-02-13T16:41:35.321449Z","iopub.status.idle":"2025-02-13T16:41:35.327295Z","shell.execute_reply.started":"2025-02-13T16:41:35.321423Z","shell.execute_reply":"2025-02-13T16:41:35.326328Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## **GCN improve**","metadata":{}},{"cell_type":"code","source":"class GCNEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n        super(GCNEncoder, self).__init__()\n        self.adj_matrix = adj_matrix\n        self.gcn1 = nn.Linear(input_dim, hidden_dim)\n        self.gcn2 = nn.Linear(hidden_dim, hidden_dim)\n        self.gcn3 = nn.Linear(hidden_dim, output_dim)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)  # BatchNorm for GCN Layer 1\n        self.bn2 = nn.BatchNorm1d(hidden_dim)  # BatchNorm for GCN Layer 2\n        self.bn3 = nn.BatchNorm1d(output_dim)  # BatchNorm for GCN Layer 3\n        \n    def forward(self, x):\n        # x shape: (batch_size, T, N, d)\n        batch_size, T, N, d = x.shape\n        \n        # Reshape for GCN: (batch_size * T, N, d)\n        x = x.view(-1, N, d)\n        \n        # GCN Layer 1\n        x = torch.matmul(self.adj_matrix, self.gcn1(x))  # Shape: (batch_size * T, N, hidden_dim)\n        x = x.view(-1, self.gcn1.out_features)  # Reshape for BatchNorm: (batch_size * T * N, hidden_dim)\n        x = self.bn1(x)  # Apply BatchNorm\n        x = x.view(-1, N, self.gcn1.out_features)  # Reshape back: (batch_size * T, N, hidden_dim)\n        x = F.relu(x)\n        \n        # GCN Layer 2\n        x = torch.matmul(self.adj_matrix, self.gcn2(x))  # Shape: (batch_size * T, N, hidden_dim)\n        x = x.view(-1, self.gcn2.out_features)  # Reshape for BatchNorm: (batch_size * T * N, hidden_dim)\n        x = self.bn2(x)  # Apply BatchNorm\n        x = x.view(-1, N, self.gcn2.out_features)  # Reshape back: (batch_size * T, N, hidden_dim)\n        x = F.relu(x)\n        \n        # GCN Layer 3\n        x = torch.matmul(self.adj_matrix, self.gcn3(x))  # Shape: (batch_size * T, N, output_dim)\n        x = x.view(-1, self.gcn3.out_features)  # Reshape for BatchNorm: (batch_size * T * N, output_dim)\n        x = self.bn3(x)  # Apply BatchNorm\n        x = x.view(-1, N, self.gcn3.out_features)  # Reshape back: (batch_size * T, N, output_dim)\n        \n        # Reshape back: (batch_size, T, N, output_dim)\n        x = x.view(batch_size, T, N, -1)\n        \n        # Aggregate over joints and time: (batch_size, output_dim)\n        x = x.mean(dim=1).mean(dim=1)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:07:14.364734Z","iopub.execute_input":"2025-02-12T18:07:14.365078Z","iopub.status.idle":"2025-02-12T18:07:14.37286Z","shell.execute_reply.started":"2025-02-12T18:07:14.365056Z","shell.execute_reply":"2025-02-12T18:07:14.371852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **GCN with dropout**","metadata":{}},{"cell_type":"code","source":"class GCNEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n        super(GCNEncoder, self).__init__()\n        self.adj_matrix = adj_matrix\n        self.gcn1 = nn.Linear(input_dim, hidden_dim)\n        self.gcn2 = nn.Linear(hidden_dim, hidden_dim)\n        self.gcn3 = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(0.5)  # Add dropout\n        \n    def forward(self, x):\n        # x shape: (batch_size, T, N, d)\n        batch_size, T, N, d = x.shape\n        \n        # Reshape for GCN: (batch_size * T, N, d)\n        x = x.view(-1, N, d)\n        \n        # GCN Layer 1\n        x = F.relu(torch.matmul(self.adj_matrix, self.gcn1(x)))\n        x = self.dropout(x)  # Apply dropout\n        \n        # GCN Layer 2\n        x = F.relu(torch.matmul(self.adj_matrix, self.gcn2(x)))\n        x = self.dropout(x)  # Apply dropout\n        \n        # GCN Layer 3\n        x = torch.matmul(self.adj_matrix, self.gcn3(x))\n        \n        # Reshape back: (batch_size, T, N, output_dim)\n        x = x.view(batch_size, T, N, -1)\n        \n        # Aggregate over joints and time: (batch_size, output_dim)\n        x = x.mean(dim=1).mean(dim=1)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T18:52:36.79963Z","iopub.execute_input":"2025-02-12T18:52:36.799924Z","iopub.status.idle":"2025-02-12T18:52:36.805616Z","shell.execute_reply.started":"2025-02-12T18:52:36.799904Z","shell.execute_reply":"2025-02-12T18:52:36.80489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## STGCN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass STGCNLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, edge_index):\n        super(STGCNLayer, self).__init__()\n        self.gcn = GCNConv(in_channels, out_channels)\n        self.temporal_conv = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.edge_index = edge_index\n        self.bn = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):\n        # Input shape: (batch_size, T, N, in_channels)\n        batch_size, T, N, in_channels = x.shape\n\n        # Spatial GCN ----------------------------------------------------------\n        x = x.reshape(batch_size * T * N, in_channels)  # (batch*T*N, in_channels)\n        x = self.gcn(x, self.edge_index)                # (batch*T*N, out_channels)\n        x = x.reshape(batch_size, T, N, -1)             # (batch, T, N, out_channels)\n\n        # Temporal Convolution --------------------------------------------------\n        x = x.permute(0, 3, 1, 2)  # (batch, out_channels, T, N)\n        x = x.reshape(batch_size, self.gcn.out_channels, T * N)\n        x = self.temporal_conv(x)  # (batch, out_channels, T*N)\n        x = x.reshape(batch_size, self.gcn.out_channels, T, N)\n        x = x.permute(0, 2, 3, 1)  # (batch, T, N, out_channels) <-- Critical fix\n\n        # Batch Normalization --------------------------------------------------\n        x = x.reshape(-1, self.gcn.out_channels)  # (batch*T*N, out_channels)\n        x = self.bn(x)\n        x = x.reshape(batch_size, T, N, -1)       # (batch, T, N, out_channels)\n        return F.relu(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:06.920874Z","iopub.execute_input":"2025-02-13T16:00:06.921236Z","iopub.status.idle":"2025-02-13T16:00:11.308810Z","shell.execute_reply.started":"2025-02-13T16:00:06.921205Z","shell.execute_reply":"2025-02-13T16:00:11.308008Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Example input\nbatch_size, T, N, d = 16, 10, 22, 3  # Input dimensions\nx = torch.randn(batch_size, T, N, d)  # Random input\n\n# Define edge_index (from previous code)\nedge_index = torch.tensor([[0, 1, 2], [1, 2, 3]], dtype=torch.long)  # Example edge_index\n\n# Initialize STGCNLayer\nst_gcn_layer = STGCNLayer(in_channels=d, out_channels=64, edge_index=edge_index)\n\n# Forward pass\noutput = st_gcn_layer(x)\nprint(output.shape)  # Should be (batch_size, out_channels, T, N)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:11.879140Z","iopub.execute_input":"2025-02-13T16:00:11.879656Z","iopub.status.idle":"2025-02-13T16:00:12.069027Z","shell.execute_reply.started":"2025-02-13T16:00:11.879619Z","shell.execute_reply":"2025-02-13T16:00:12.068060Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 10, 22, 64])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass STGCNEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, edge_index):\n        super(STGCNEncoder, self).__init__()\n        self.output_dim = output_dim\n        self.edge_index = edge_index\n        \n        # ST-GCN layers\n        self.st_gcn1 = STGCNLayer(input_dim, hidden_dim, edge_index)\n        self.st_gcn2 = STGCNLayer(hidden_dim, hidden_dim, edge_index)\n        self.st_gcn3 = STGCNLayer(hidden_dim, output_dim, edge_index)\n        \n    def forward(self, x):\n        # Input shape: (batch_size, T, N, input_dim)\n        x = self.st_gcn1(x)  # Output: (batch, T, N, hidden_dim)\n        residual = x\n        x = self.st_gcn2(x)  # Output: (batch, T, N, hidden_dim)\n        x = x + residual     # Residual connection\n        x = self.st_gcn3(x)  # Output: (batch, T, N, output_dim)\n        \n        # Aggregate over time (T) and joints (N)\n        x = x.mean(dim=1).mean(dim=1)  # Shape: (batch_size, output_dim)\n        return x  # Fixed: Added layer\n        \n    \"\"\"def forward(self, x):\n        # x shape: (batch_size, T, N, d)\n        print(\"Input shape:\", x.shape)  # Debug\n        x = self.st_gcn1(x)\n        print(\"After st_gcn1:\", x.shape)  # Should be (16, 64, 10, 22)\n        residual = x\n        x = self.st_gcn2(x)\n        print(\"After st_gcn2:\", x.shape)  # Should be (16, 64, 10, 22)\n        x = x + residual  # Residual connection\n        print(\"After residual:\", x.shape)  # Should be (16, 64, 10, 22)\n        x = self.st_gcn3(x)\n        print(\"After st_gcn3:\", x.shape)  # Should be (16, 128, 10, 22)\n        x = x.mean(dim=2).mean(dim=2)  # Aggregate over T and N\n        print(\"Final output:\", x.shape)  # Should be (16, 128)\n        return x\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:14.161434Z","iopub.execute_input":"2025-02-13T16:00:14.161828Z","iopub.status.idle":"2025-02-13T16:00:14.168219Z","shell.execute_reply.started":"2025-02-13T16:00:14.161796Z","shell.execute_reply":"2025-02-13T16:00:14.167279Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Test input\nbatch_size, T, N, d = 16, 10, 22, 3\nx = torch.randn(batch_size, T, N, d)\n\n# Define edge_index (replace with your kinematic tree)\nedges = []\nkinematic_tree = [[0, 2, 5, 8, 11], [0, 1, 4, 7, 10], [0, 3, 6, 9, 12, 15], [9, 14, 17, 19, 21], [9, 13, 16, 18, 20]]\nfor branch in kinematic_tree:\n    for i in range(len(branch) - 1):\n        edges.append([branch[i], branch[i+1]])\n        edges.append([branch[i+1], branch[i]])  # Undirected edges\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n\n# Initialize encoder\nencoder = STGCNEncoder(input_dim=3, hidden_dim=64, output_dim=128, edge_index=edge_index)\n\n# Forward pass\noutput = encoder(x)\nprint(\"Encoder output shape:\", output.shape)  # Should be (16, 128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:17.077385Z","iopub.execute_input":"2025-02-13T16:00:17.077752Z","iopub.status.idle":"2025-02-13T16:00:17.120832Z","shell.execute_reply.started":"2025-02-13T16:00:17.077722Z","shell.execute_reply":"2025-02-13T16:00:17.119882Z"}},"outputs":[{"name":"stdout","text":"Encoder output shape: torch.Size([16, 128])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import numpy as np\n\n# Define kinematic_tree\nkinematic_tree = [\n    [0, 2, 5, 8, 11], \n    [0, 1, 4, 7, 10], \n    [0, 3, 6, 9, 12, 15], \n    [9, 14, 17, 19, 21], \n    [9, 13, 16, 18, 20]\n]\n\n# Convert to edge indices\nedges = []\nfor branch in kinematic_tree:\n    for i in range(len(branch) - 1):\n        edges.append([branch[i], branch[i + 1]])\n        edges.append([branch[i + 1], branch[i]])  # Undirected graph\n\nedge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:19.198181Z","iopub.execute_input":"2025-02-13T16:00:19.198561Z","iopub.status.idle":"2025-02-13T16:00:19.204604Z","shell.execute_reply.started":"2025-02-13T16:00:19.198527Z","shell.execute_reply":"2025-02-13T16:00:19.203572Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## **Motion to text Model for simple GCN**","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\nclass MotionToTextModel(nn.Module):\n    def __init__(self, gcn_encoder, t5_model_name='t5-small'):\n        super(MotionToTextModel, self).__init__()\n        self.gcn_encoder = gcn_encoder\n        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n        self.tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n        \n        # Project motion encoding to T5 embedding dimension\n        t5_embedding_dim = self.t5.config.d_model\n        self.projection = nn.Linear(gcn_encoder.gcn2.out_features, t5_embedding_dim)\n        \n    def forward(self, motion, target_text=None):\n        # Encode motion: (batch_size, output_dim)\n        motion_encoded = self.gcn_encoder(motion)\n        \n        # Project motion encoding to T5 embedding dimension: (batch_size, t5_embedding_dim)\n        motion_encoded = self.projection(motion_encoded)\n        \n        # Prepare input for T5\n        input_ids = self.tokenizer(\n            \"motion to text: \", return_tensors=\"pt\"\n        ).input_ids.to(motion.device)\n        \n        # Repeat input_ids for the batch size\n        input_ids = input_ids.repeat(motion.size(0), 1)\n        \n        # Get T5 input embeddings: (batch_size, sequence_length, t5_embedding_dim)\n        decoder_inputs_embeds = self.t5.get_input_embeddings()(input_ids)\n        \n        # Concatenate motion encoding with T5 input embeddings\n        motion_encoded = motion_encoded.unsqueeze(1)  # (batch_size, 1, t5_embedding_dim)\n        decoder_inputs_embeds = torch.cat([motion_encoded, decoder_inputs_embeds], dim=1)\n        \n        # Generate text\n        if target_text is not None:\n            # Training mode\n            labels = self.tokenizer(\n                target_text, return_tensors=\"pt\", padding=True, truncation=True\n            ).input_ids.to(motion.device)\n            outputs = self.t5(inputs_embeds=decoder_inputs_embeds, labels=labels)\n            return outputs.loss\n        else:\n            # Inference mode\n            generated_ids = self.t5.generate(inputs_embeds=decoder_inputs_embeds)\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:41:47.340014Z","iopub.execute_input":"2025-02-13T16:41:47.340353Z","iopub.status.idle":"2025-02-13T16:41:47.347839Z","shell.execute_reply.started":"2025-02-13T16:41:47.340322Z","shell.execute_reply":"2025-02-13T16:41:47.346885Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## **Motion to text model for multilayer GCN**","metadata":{}},{"cell_type":"code","source":"class MotionToTextModel(nn.Module):\n    def __init__(self, gcn_encoder, t5_model_name='t5-small'):\n        super(MotionToTextModel, self).__init__()\n        self.gcn_encoder = gcn_encoder\n        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n        self.tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n        \n        # Project motion encoding to T5 embedding dimension\n        t5_embedding_dim = self.t5.config.d_model\n        self.projection = nn.Linear(gcn_encoder.gcn3.out_features, t5_embedding_dim)  # Use gcn3.out_features\n        \n    def forward(self, motion, target_text=None):\n        # Encode motion: (batch_size, output_dim)\n        motion_encoded = self.gcn_encoder(motion)\n        \n        # Project motion encoding to T5 embedding dimension: (batch_size, t5_embedding_dim)\n        motion_encoded = self.projection(motion_encoded)\n        \n        # Prepare input for T5\n        input_ids = self.tokenizer(\n            \"motion to text: \", return_tensors=\"pt\"\n        ).input_ids.to(motion.device)\n        \n        # Repeat input_ids for the batch size\n        input_ids = input_ids.repeat(motion.size(0), 1)\n        \n        # Get T5 input embeddings: (batch_size, sequence_length, t5_embedding_dim)\n        decoder_inputs_embeds = self.t5.get_input_embeddings()(input_ids)\n        \n        # Concatenate motion encoding with T5 input embeddings\n        motion_encoded = motion_encoded.unsqueeze(1)  # (batch_size, 1, t5_embedding_dim)\n        decoder_inputs_embeds = torch.cat([motion_encoded, decoder_inputs_embeds], dim=1)\n        \n        # Generate text\n        if target_text is not None:\n            # Training mode\n            labels = self.tokenizer(\n                target_text, return_tensors=\"pt\", padding=True, truncation=True\n            ).input_ids.to(motion.device)\n            outputs = self.t5(inputs_embeds=decoder_inputs_embeds, labels=labels)\n            return outputs.loss\n        else:\n            # Inference mode\n            generated_ids = self.t5.generate(inputs_embeds=decoder_inputs_embeds)\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:24.314453Z","iopub.execute_input":"2025-02-13T16:00:24.314886Z","iopub.status.idle":"2025-02-13T16:00:24.322602Z","shell.execute_reply.started":"2025-02-13T16:00:24.314852Z","shell.execute_reply":"2025-02-13T16:00:24.321539Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Motion To Text for STGCN","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nclass MotionToTextModel(nn.Module):\n    def __init__(self, gcn_encoder, t5_model_name='t5-small'):\n        super(MotionToTextModel, self).__init__()\n        self.gcn_encoder = gcn_encoder\n        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n        self.tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n        \n        # Project motion encoding to T5 embedding dimension\n        t5_embedding_dim = self.t5.config.d_model\n        self.projection = nn.Linear(gcn_encoder.output_dim, t5_embedding_dim)\n        \n    def forward(self, motion, target_text=None):\n        # Encode motion: (batch_size, output_dim)\n        motion_encoded = self.gcn_encoder(motion)\n        \n        # Project motion encoding to T5 embedding dimension: (batch_size, t5_embedding_dim)\n        motion_encoded = self.projection(motion_encoded)\n        \n        # Prepare T5 input\n        input_text = \"motion to text: \"\n        input_ids = self.tokenizer(\n            input_text, return_tensors=\"pt\", padding=True, truncation=True\n        ).input_ids.to(motion.device)\n        \n        # Repeat input_ids for the batch size\n        input_ids = input_ids.repeat(motion.size(0), 1)\n        \n        # Get T5 input embeddings: (batch_size, sequence_length, t5_embedding_dim)\n        inputs_embeds = self.t5.get_input_embeddings()(input_ids)\n        \n        # Concatenate motion encoding with T5 input embeddings\n        motion_encoded = motion_encoded.unsqueeze(1)  # (batch_size, 1, t5_embedding_dim)\n        inputs_embeds = torch.cat([motion_encoded, inputs_embeds], dim=1)\n        \n        if target_text is not None:\n            # Training mode\n            labels = self.tokenizer(\n                target_text, return_tensors=\"pt\", padding=True, truncation=True\n            ).input_ids.to(motion.device)\n            \n            # Forward pass with labels\n            outputs = self.t5(inputs_embeds=inputs_embeds, labels=labels)\n            return outputs.loss\n        else:\n            # Inference mode\n            generated_ids = self.t5.generate(inputs_embeds=inputs_embeds)\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n            return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:41.357279Z","iopub.execute_input":"2025-02-13T16:00:41.357666Z","iopub.status.idle":"2025-02-13T16:00:57.467801Z","shell.execute_reply.started":"2025-02-13T16:00:41.357623Z","shell.execute_reply":"2025-02-13T16:00:57.466923Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def augment_motion(motion, noise_scale=0.01):\n    noise = torch.randn_like(motion) * noise_scale\n    return motion + noise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:00:57.468937Z","iopub.execute_input":"2025-02-13T16:00:57.469545Z","iopub.status.idle":"2025-02-13T16:00:57.474030Z","shell.execute_reply.started":"2025-02-13T16:00:57.469505Z","shell.execute_reply":"2025-02-13T16:00:57.472845Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import numpy as np\n# Hyperparameters\ninput_dim = 3  # x, y, z coordinates\nhidden_dim = 64\noutput_dim = 128\nbatch_size = 64\nlearning_rate = 1e-4\nnum_epochs = 35\n\n# Initialize model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move adj_matrix to the device\nadj_matrix = adj_matrix.to(device)\n\n# Initialize GCN encoder and model\ngcn_encoder = GCNEncoder(input_dim, hidden_dim, output_dim, adj_matrix)\nmodel = MotionToTextModel(gcn_encoder).to(device)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for motion, text in train_loader:\n        if np.random.random() < 0.5:\n            motion = augment_motion(motion, noise_scale=0.01)\n        # Move data to device\n        motion = motion.to(device)\n        \n        # Forward pass\n        loss = model(motion, text)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:46:16.441554Z","iopub.execute_input":"2025-02-13T16:46:16.441863Z","iopub.status.idle":"2025-02-13T17:07:35.167667Z","shell.execute_reply.started":"2025-02-13T16:46:16.441839Z","shell.execute_reply":"2025-02-13T17:07:35.166744Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.4037609100341797\nEpoch 2, Loss: 1.309475064277649\nEpoch 3, Loss: 1.7307119369506836\nEpoch 4, Loss: 0.7791255712509155\nEpoch 5, Loss: 0.977437436580658\nEpoch 6, Loss: 1.1363563537597656\nEpoch 7, Loss: 1.036194086074829\nEpoch 8, Loss: 1.1051623821258545\nEpoch 9, Loss: 1.4576685428619385\nEpoch 10, Loss: 1.4136892557144165\nEpoch 11, Loss: 0.7947259545326233\nEpoch 12, Loss: 1.2465604543685913\nEpoch 13, Loss: 1.3981118202209473\nEpoch 14, Loss: 1.1476515531539917\nEpoch 15, Loss: 1.2456958293914795\nEpoch 16, Loss: 0.9917926788330078\nEpoch 17, Loss: 1.1449682712554932\nEpoch 18, Loss: 1.16427743434906\nEpoch 19, Loss: 1.1025314331054688\nEpoch 20, Loss: 0.9347779154777527\nEpoch 21, Loss: 1.597883939743042\nEpoch 22, Loss: 1.0885844230651855\nEpoch 23, Loss: 1.3894978761672974\nEpoch 24, Loss: 1.030968427658081\nEpoch 25, Loss: 1.1392749547958374\nEpoch 26, Loss: 1.258576512336731\nEpoch 27, Loss: 0.9019225835800171\nEpoch 28, Loss: 1.009253740310669\nEpoch 29, Loss: 0.8514111042022705\nEpoch 30, Loss: 1.0696792602539062\nEpoch 31, Loss: 0.97529536485672\nEpoch 32, Loss: 1.1098536252975464\nEpoch 33, Loss: 1.0860298871994019\nEpoch 34, Loss: 1.0684020519256592\nEpoch 35, Loss: 0.9085702896118164\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Training for STGCN","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\n\n\n# Hyperparameters\ninput_dim = 3  # x, y, z coordinates\nhidden_dim = 64\noutput_dim = 128\nbatch_size = 64\nlearning_rate = 1e-4\nnum_epochs = 45\n\n# Initialize model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move edge_index to the device\nedge_index = edge_index.to(device)\n\n# Initialize ST-GCN encoder and model\nst_gcn_encoder = STGCNEncoder(input_dim, hidden_dim, output_dim, edge_index)\nmodel = MotionToTextModel(st_gcn_encoder).to(device)\n\n# Initialize optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # Add weight decay\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR by 0.1 every 10 epochs\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for motion, text in train_loader:\n        if np.random.random() < 0.5:\n            motion = augment_motion(motion, noise_scale=0.01)\n        # Move data to device\n        motion = motion.to(device)\n        \n        # Forward pass\n        loss = model(motion, text)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:03:16.610202Z","iopub.execute_input":"2025-02-13T16:03:16.610564Z","iopub.status.idle":"2025-02-13T16:35:24.825155Z","shell.execute_reply.started":"2025-02-13T16:03:16.610535Z","shell.execute_reply":"2025-02-13T16:35:24.824324Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.9364256858825684\nEpoch 2, Loss: 1.1304962635040283\nEpoch 3, Loss: 1.2142441272735596\nEpoch 4, Loss: 1.556441307067871\nEpoch 5, Loss: 1.1923068761825562\nEpoch 6, Loss: 0.9270095825195312\nEpoch 7, Loss: 1.2069671154022217\nEpoch 8, Loss: 0.9979519844055176\nEpoch 9, Loss: 1.3539304733276367\nEpoch 10, Loss: 1.0229213237762451\nEpoch 11, Loss: 1.2794729471206665\nEpoch 12, Loss: 1.0508111715316772\nEpoch 13, Loss: 1.190352201461792\nEpoch 14, Loss: 0.9436461925506592\nEpoch 15, Loss: 1.4238595962524414\nEpoch 16, Loss: 1.102981686592102\nEpoch 17, Loss: 1.214152455329895\nEpoch 18, Loss: 0.8955529928207397\nEpoch 19, Loss: 1.2250924110412598\nEpoch 20, Loss: 1.0722805261611938\nEpoch 21, Loss: 0.7945737242698669\nEpoch 22, Loss: 0.776954174041748\nEpoch 23, Loss: 1.018916368484497\nEpoch 24, Loss: 0.7210644483566284\nEpoch 25, Loss: 1.3190574645996094\nEpoch 26, Loss: 0.9304620623588562\nEpoch 27, Loss: 1.0007092952728271\nEpoch 28, Loss: 1.318511724472046\nEpoch 29, Loss: 1.180087685585022\nEpoch 30, Loss: 1.119889259338379\nEpoch 31, Loss: 1.039164423942566\nEpoch 32, Loss: 1.2349497079849243\nEpoch 33, Loss: 1.323854684829712\nEpoch 34, Loss: 1.0690739154815674\nEpoch 35, Loss: 1.2169114351272583\nEpoch 36, Loss: 1.1777149438858032\nEpoch 37, Loss: 1.0928281545639038\nEpoch 38, Loss: 1.5067530870437622\nEpoch 39, Loss: 1.0595180988311768\nEpoch 40, Loss: 1.041104793548584\nEpoch 41, Loss: 0.6504679322242737\nEpoch 42, Loss: 1.2288801670074463\nEpoch 43, Loss: 1.0788969993591309\nEpoch 44, Loss: 1.5282301902770996\nEpoch 45, Loss: 1.4155564308166504\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Training with optimized learning_rate","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\n\n# Hyperparameters\ninput_dim = 3  # x, y, z coordinates\nhidden_dim = 64\noutput_dim = 128\nbatch_size = 64\nlearning_rate = 1e-4\nnum_epochs = 20\n\n# Initialize model and move to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move adj_matrix to the device\nadj_matrix = adj_matrix.to(device)\n\n# Initialize GCN encoder and model\ngcn_encoder = GCNEncoder(input_dim, hidden_dim, output_dim, adj_matrix)\nmodel = MotionToTextModel(gcn_encoder).to(device)\n\n# Initialize optimizer and scheduler\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # Add weight decay\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR by 0.1 every 10 epochs\n\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for motion, text in train_loader:\n        # Move data to device\n        motion = motion.to(device)\n        \n        # Forward pass\n        loss = model(motion, text)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T08:49:46.918522Z","iopub.execute_input":"2025-02-13T08:49:46.918853Z","iopub.status.idle":"2025-02-13T08:49:47.718627Z","shell.execute_reply.started":"2025-02-13T08:49:46.918825Z","shell.execute_reply":"2025-02-13T08:49:47.717476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    count = 0\n    for motion, text in valid_loader:\n        \n        if count > 10 :\n            break\n        motion = motion.to(device)\n        generated_text = model(motion)\n        print(\"Generated Text:\", generated_text[0])\n        print(\"Ground Truth Text:\", text[0])\n        count += 1\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T17:07:52.448596Z","iopub.execute_input":"2025-02-13T17:07:52.448910Z","iopub.status.idle":"2025-02-13T17:07:56.363067Z","shell.execute_reply.started":"2025-02-13T17:07:52.448885Z","shell.execute_reply":"2025-02-13T17:07:56.362114Z"}},"outputs":[{"name":"stdout","text":"Generated Text: a person is sitting down and then stands back up.\nGround Truth Text: the man has crossed his legs and sat down\nGenerated Text: a person walks in a circle, turns around and walks back.\nGround Truth Text: the person walks in a straight line at a angle to their left, then turns around and jogs back to the start.\nGenerated Text: a person walks backwards, then turns around and walks backwards.\nGround Truth Text: a person is spinning slowly in place, reaching hands out occasionally.\nGenerated Text: a person walks in a counterclockwise circle.\nGround Truth Text: person walks to the left, opens something and then walks back\nGenerated Text: a person walks backwards, then turns around and walks backwards.\nGround Truth Text: a person ice skating in a circle\nGenerated Text: a person walks forward, turns around and walks back.\nGround Truth Text: a man walks slowly forward.\nGenerated Text: a person sits down and stands back up.\nGround Truth Text: a person is swimming slowly.\nGenerated Text: a person walks in a circle.\nGround Truth Text: someone walks to the right side, gets on a wall and jumps, then keeps walking\nGenerated Text: a person walks forward, turns around and walks back.\nGround Truth Text: the person walks forward and then waits while its arms waves.\nGenerated Text: a person walks forward, turns around and walks back.\nGround Truth Text: a person walks forward and to the left then sits down on something.\nGenerated Text: a person is sitting down and then stands back up.\nGround Truth Text: person moves back sits down then gets up and moves forward\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import csv\nmotion_dir = \"/kaggle/input/human-motion-description-hmd-motion-to-text/motions/\"  # Folder containing .npy files\ntest_file = \"/kaggle/input/human-motion-description-hmd-motion-to-text/test.txt\"  # List of motion IDs\noutput_csv = \"submission.csv\"  # Output CSV\ncaptions = [[\"id\", \"text\"]]  # Store results\nwith open(test_file, \"r\") as f:\n    motion_ids = f.read().splitlines()  # Read all motion IDs\nfor motion_id in tqdm(motion_ids, desc=\"Processing motions\"):\n    model.train()\n    motion_path = os.path.join(motion_dir, motion_id + \".npy\")\n    \n    if not os.path.exists(motion_path):\n        print(f\" Motion file {motion_id} not found. Skipping...\")\n        continue\n    \n    \n    motion = np.load(os.path.join(motion_data_dir, motion_path))\n    motion = torch.from_numpy(motion).unsqueeze(0)\n    motion = motion.to(device)\n    generated_text = model(motion)\n    # Save result\n    captions.append([motion_id, generated_text])\n\nwith open(output_csv, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerows(captions)\n\nprint(f\"\\n Motion captions saved to {output_csv}!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:36:39.831209Z","iopub.execute_input":"2025-02-13T16:36:39.831547Z","iopub.status.idle":"2025-02-13T16:39:14.389870Z","shell.execute_reply.started":"2025-02-13T16:36:39.831514Z","shell.execute_reply":"2025-02-13T16:39:14.389072Z"}},"outputs":[{"name":"stderr","text":"Processing motions: 100%|██████████| 1000/1000 [02:34<00:00,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"\n Motion captions saved to submission.csv!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nsub = pd.read_csv('/kaggle/working/submission.csv')\nsub[\"text\"] = sub[\"text\"].str.strip(\"[]'\")\nsub.to_csv('submission_.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:39:18.452775Z","iopub.execute_input":"2025-02-13T16:39:18.453066Z","iopub.status.idle":"2025-02-13T16:39:18.473110Z","shell.execute_reply.started":"2025-02-13T16:39:18.453043Z","shell.execute_reply":"2025-02-13T16:39:18.472448Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"sub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T16:39:21.904644Z","iopub.execute_input":"2025-02-13T16:39:21.904942Z","iopub.status.idle":"2025-02-13T16:39:21.925289Z","shell.execute_reply.started":"2025-02-13T16:39:21.904918Z","shell.execute_reply":"2025-02-13T16:39:21.924551Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"          id                                               text\n0     014295           a person walks forward and turns around.\n1     005166  a person walks forward, turns around and walks...\n2    M002388  a person walks forward and then turns around a...\n3    M007286                     a person walks forward slowly.\n4     000304  a person walks forward, then turns around and ...\n..       ...                                                ...\n995  M011372  a person walks forward and turns around and wa...\n996  M007007  a person walks forward and then turns to the l...\n997  M010671  a person walks forward and then turns around a...\n998   001038  a person walks forward, turns around, and walk...\n999   011091  a person walks forward, turns around and walks...\n\n[1000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>014295</td>\n      <td>a person walks forward and turns around.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>005166</td>\n      <td>a person walks forward, turns around and walks...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M002388</td>\n      <td>a person walks forward and then turns around a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M007286</td>\n      <td>a person walks forward slowly.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000304</td>\n      <td>a person walks forward, then turns around and ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>M011372</td>\n      <td>a person walks forward and turns around and wa...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>M007007</td>\n      <td>a person walks forward and then turns to the l...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>M010671</td>\n      <td>a person walks forward and then turns around a...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>001038</td>\n      <td>a person walks forward, turns around, and walk...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>011091</td>\n      <td>a person walks forward, turns around and walks...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"### **Evaluation Metric `Meteor-Score`**","metadata":{}},{"cell_type":"code","source":"\"\"\"\nText Generation Evaluation Metric (Meteor Score)\n\nEvaluates submissions using meteor score to reference texts\n\"\"\"\nimport bisect\nfrom collections import defaultdict\n\ndef compute_lis(arr):\n    \"\"\"Compute longest increasing subsequence (O(n log n) time).\"\"\"\n    tails = []\n    for num in arr:\n        idx = bisect.bisect_left(tails, num)\n        if idx == len(tails):\n            tails.append(num)\n        else:\n            tails[idx] = num\n    return tails\n\ndef _calculate_chunks(reference_unigrams, candidate_unigrams):\n    \"\"\"Optimized chunk calculation using LIS.\"\"\"\n    # Create inverted index for reference words\n    word_to_ref_indices = defaultdict(list)\n    for idx, word in enumerate(reference_unigrams):\n        word_to_ref_indices[word].append(idx)\n    \n    # Collect all matching positions\n    matches = []\n    for c_idx, word in enumerate(candidate_unigrams):\n        if word in word_to_ref_indices:\n            matches.extend((r_idx, c_idx) for r_idx in word_to_ref_indices[word])\n    \n    if not matches:\n        return 0, 0\n    \n    # Sort matches by reference index then candidate index\n    matches.sort(key=lambda x: (x[0], x[1]))\n    cand_indices = [c_idx for _, c_idx in matches]\n    \n    # Get LIS of candidate indices\n    lis = compute_lis(cand_indices)\n    if not lis:\n        return 0, 0\n    \n    # Calculate number of chunks\n    chunks = 1\n    for i in range(1, len(lis)):\n        if lis[i] != lis[i-1] + 1:\n            chunks += 1\n    \n    return chunks, len(lis)\n\ndef meteor(reference, candidate):\n    \"\"\"Optimized METEOR score calculation.\"\"\"\n    ref_tokens = reference.split()\n    can_tokens = candidate.split()\n    \n    # Fast intersection check\n    ref_words = set(ref_tokens)\n    if not any(word in ref_words for word in can_tokens):\n        return 0.0\n    \n    # Calculate precision/recall\n    m = sum(1 for word in can_tokens if word in ref_words)\n    precision = m / len(can_tokens) if can_tokens else 0\n    recall = m / len(ref_tokens) if ref_tokens else 0\n    \n    # Handle edge cases\n    if m == 0:\n        return 0.0\n    \n    # Harmonic mean\n    f_mean = (10 * precision * recall) / (recall + 9 * precision) if (precision + recall) > 0 else 0\n    \n    # Penalty calculation\n    chunks, mappings = _calculate_chunks(ref_tokens, can_tokens)\n    penalty = 0.5 * (chunks / mappings) ** 3\n    \n    return min(f_mean * (1 - penalty), 1.0)\n\n\ndef get_meteor_score(references, candidate):\n    \n    return max([meteor(reference, candidate) for reference in references])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Usage Example\nreference_texts = [\"a person walks aimlessly and slowly in an imperfect circle around the room, lathargecly swaying their arms with each step.\",\n                   \"a person walking with their arms swinging back to front and walking in a general circle.\",\n                   \"a person walks in an oval path and ends where he started.\",\n                   ]\npredicted_text = \"a person walks in a circle path swinging with arms.\"\n\nget_meteor_score(reference_texts, predicted_text)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Code to generate your submission `.csv` file**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random, string\n\n## /!\\ RED ALERT STORM\n## This is just to generate random text to show you an example of submission\n## In your case, you have to predict the texts using your trained model !\ndef generate_random_text(length):\n    \"\"\"generates random sentences\"\"\"\n    letters = string.ascii_lowercase + '     '\n    return ''.join(random.choice(letters) for i in range(length))\n\n## /!\\ alerte rouge ! vents -> replace this with your actual predictions\ntest_texts_ids = np.arange(0, 1000).astype('str') # list of test texts ids\npred_test_texts = [generate_random_text(30) for i in range(1000)] ## pred_test_text [numpy array or list] is your predicted texts #shape: (1000,), /!\\ in the same order as the ids !\n\n## create submission dataframe\nsubmission_df = pd.DataFrame({'id': test_texts_ids,\n                              'text': pred_test_texts})\nsubmission_df.to_csv('./submission.csv', index=False)\nsubmission_df","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Charger le fichier .npy\nimage = np.load(\"./motions\")\n\n# Afficher la forme de l'image\nprint(f\"Shape de l'image : {image.shape}\")\n\n# Afficher l'image\nplt.imshow(image, cmap=\"gray\" if len(image.shape) == 2 else None)\nplt.axis(\"off\")  # Supprimer les axes\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null}]}